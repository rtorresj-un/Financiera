---
title: "Parcial 3 - Econometría Financiera"
subtitle: "Raúl Esteban Torres Jiménez & Juanita Cortés Arroyo"
output: 
  pdf_document: 
    latex_engine: xelatex
---

```{r, include=FALSE}
library(quantmod); library(PerformanceAnalytics); library(rugarch); library(dplyr)
library(ggplot2); library(ggfortify); library(quantmod); library(gridExtra); library(forecast); library(cowplot); library(urca); library(timeDate); library(TSstudio); library(fBasics); library(randtests); library(vrtest); library(texreg); library(fGarch); library(urca)
library(xts);library(zoo); library(TTR);library(tidyverse);library(dplyr); library(stats); library(greybox); library(smooth);library(PerformanceAnalytics); library(rugarch); library(tseries); library(stargazer)

extract.rugarch <- function(fit, indep, 
                            include.rsquared = TRUE, include.loglike = TRUE, include.aic = TRUE, include.bic = TRUE) {

  # extract coefficient table from fit:
  coefnames <- rownames(as.data.frame(fit@fit$coef))
  coefs <- fit@fit$coef
  se <- as.vector(fit@fit$matcoef[, c(2)])
  pvalues <-  as.vector(fit@fit$matcoef[, c(4)])       # numeric vector with p-values

  # create empty GOF vectors and subsequently add GOF statistics from model:
  gof <- numeric()
  gof.names <- character()
  gof.decimal <- logical()
  if (include.rsquared == TRUE) {
    r2 <-  1 - (var(fit@fit$residuals) / var(indep))
    gof <- c(gof, r2)
    gof.names <- c(gof.names, "R^2")
    gof.decimal <- c(gof.decimal, TRUE)
  }
  if (include.loglike == TRUE) {
    loglike <- fit@fit$LLH
    gof <- c(gof, loglike)
    gof.names <- c(gof.names, "Log likelihood")
    gof.decimal <- c(gof.decimal, TRUE)
  }
  if (include.aic == TRUE) {
    aic <- infocriteria(fit)[c(1)]
    gof <- c(gof, aic)
    gof.names <- c(gof.names, "AIC")
    gof.decimal <- c(gof.decimal, TRUE)
  }

  if (include.bic == TRUE) {
    bic <- infocriteria(fit)[c(2)]
    gof <- c(gof, bic)
    gof.names <- c(gof.names, "BIC")
    gof.decimal <- c(gof.decimal, TRUE)
  }

  # create texreg object:
  tr <- createTexreg(
    coef.names = coefnames, 
    coef = coefs,
    se = se,
    pvalues = pvalues, 
    gof.names = gof.names, 
    gof = gof, 
    gof.decimal = gof.decimal
  )
  return(tr)
}


getSymbols('USDJPY=X',from = "2005-01-01", to = "2021-08-31", periodicity = "daily")
jp <-Cl(`USDJPY=X`)

jp_r <- data.frame(apply(jp, 2, function(x) Delt(x, type = "log")), fecha = index(jp))
jp_r<-na.omit(jp_r)
jp_r<-xts(jp_r[,1], order.by = jp_r$fecha)
jp_r08<-window(jp_r, start='2008-01-21', end='2009-12-31')
jp_r20<-window(jp_r, start='2020-01-01', end=Sys.Date())
```

Análisis del comportamiento de la tasa de cambio del dólar americano USD frente al yen japones (USD/JPY).   

  a. Estadística descriptiva (Además de otros análisis calcule la desviación estándar anualizada para los años de crisis (2008 y 2009), también la del año 2020 y comparela con la de la muestra completa.)
  
Se presenta la gráfica de la tasa de cambio del dolar americado \textbf{USD} frente al Yen japones \textbf{JPY} con datos desde enero de 2005 hasta agosto del 2021.

```{r echo=FALSE ,message=FALSE ,fig.height = 2, fig.width = 7, fig.align = "center", fig.pos='h'}
knitr::opts_chunk$set(fig.width=4) 
grid.arrange(
ggplot(data = jp, aes(y = jp, x = index(jp))) + geom_line()+
  ggtitle("Tasa de cambio USD/JPY")+ 
  labs(x = "Fecha",y = "USD/JPY") +
  theme_minimal(), ncol=2,
  ggAcf(jp,52)+ggtitle("USD/JPY")+labs(x="")+theme_minimal())
```
La serie de la tasa de cambio no presenta una tendencia clara, sin embargo, presenta alta persistencia en el tiempo del componente autorregresivo, por lo tanto no es estacionaria.



```{r echo=FALSE, fig.height = 2, fig.width = 7, fig.align = "center", message=FALSE,fig.pos='h'}
# plot retornos diarios 
grid.arrange(
ggplot(data = jp_r, aes(y = jp_r[,1], x = index(jp_r))) + geom_line()+ggtitle("Retornos diarios USD/JPY")+ labs(x = "Fecha",y = "") + theme_minimal(),ncol=2,
ggplot(data = jp_r, aes(y=..density.., x = jp_r[,1])) +
  geom_histogram(bins = 200) +
  ggtitle("Distrubución retornos diarios") +
  geom_vline(aes(xintercept=mean(jp_r)), linetype="dashed") +
  geom_density(alpha=.2, fill="lightblue") + labs(x='',y='') +theme_minimal()
)
```


```{r echo=FALSE, caption='Estadística descriptiva.', warning = FALSE,fig.pos='h',message=FALSE}
stats_d<-basicStats(jp_r[,1])[c("Mean", "Stdev", "Minimum", "Maximum", "Variance","Skewness","Kurtosis"),]

#En tabla: 
stats<-data.frame(stats_d)
row.names(stats)<-c("Media", "SD", "Mínimo", "Máximo", "Varianza","Asimetría","Curtosis")
colnames(stats)<-c('USD/JPY')
knitr::kable(stats)
```

El valor de la desviación estándar es más alto al de la media, por lo que la volatidad domina la dinámica de los retornos y la distribución es negativamente asimétrica. Así mismo, se observa que los retornos fluctuan alrededor de cero, son volátiles y presentan clusters de volatilidad. Además, los datos están altamente concentrados en su valor medio y se encuentra una mayor probabilidad de datos atípicos, lo que hace que la distribución de los retornos sea leptocúrtica y tenga colas pesadas, de este modo podemos concluir que la tasa de cambio sigue el comportamiento típico de los retornos de las series financieras.

```{r, echo=FALSE,fig.pos='h'}
test_ur<-NULL
test_ur[1]<-list(ur.df(jp_r, type="drift", selectlags = "AIC")@teststat)
test_ur[2]<-list(ur.df(jp_r^2, type="drift", selectlags = "AIC")@teststat)

test_ur<-cbind(c("Retornos", "Volatilidad"),c(test_ur[[1]][1], test_ur[[1]][2]), c(test_ur[[2]][1], test_ur[[2]][2]))
colnames(test_ur)<-c('Variables','Raíz unitaria', 'Deriva')

cval_ur<-data.frame(ur.df(jp_r, type="drift", selectlags = "AIC")@cval)
colnames(cval_ur)<-c('1pct','5pct','10pct')
rownames(cval_ur)<-c('Raíz unitaria', 'Deriva')
knitr::kable(test_ur)
knitr::kable(cval_ur)

#Necesidad de modelar la media del proceso.
```

```{r echo=FALSE, fig.align="center", fig.height=3, fig.pos='h', fig.width=7, message=FALSE, warning=FALSE}
grid.arrange(
ggAcf(jp_r,52)+ggtitle("Retornos diarios")+labs(x="")+theme_minimal(),
ggAcf(jp_r^2,52)+ggtitle("Volatilidad retornos diarios")+labs(x="")+theme_minimal(),
ggPacf(jp_r,52)+ggtitle("")+theme_minimal(),
ggPacf(jp_r^2,52)+ggtitle("")+theme_minimal()

)
#Necesidad de modelar la media y la varianza.
```
En cuanto a la estacionariedad, bajo la prueba de DF se concluye que la serie de los retornos es estacionaria y no presenta deriva. 
Con respecto a la autocorrelación simple y parcial de los $retornos$, no se observa un proceso altamente persistente en el tiempo lo que concuerda con la prueba de DF, sin embargo, no se puede determinar a simple vista el proceso generador de los retornos dado el comportamiento irregular de las funciones. 
En cuanto a la autocorrelación de los $retornos^2$ se observa una estructura de autocorrelación parcial, por lo que es necesario modelar este comportamiento. 


Comparando la volatilidad anualizada para diferentes periodos de la muestra observamos lo siguiente:
```{r echo=FALSE, fig.height = 6, fig.width = 7, fig.align = "center", warning = FALSE,fig.pos='h',message=FALSE, ref.label="1"}
#chart.TimeSeries(jp)
#rollapply(jp_r)
par(mfrow=c(3,1))
chart.RollingPerformance(jp_r08, width = 22, FUN = "sd.annualized", scale = 252, main = "Volatilidad anualizada 2008-2009")
chart.RollingPerformance(jp_r20, width = 22, FUN = "sd.annualized", scale = 252, main = "Volatilidad anualizada 2020-2021")
chart.RollingPerformance(jp_r, width = 22, FUN = "sd.annualized", scale = 252, main = "Volatilidad anualizada total")

#Se observan clústeres de volatilidad, media que no es cero ni varianza constante => modelar volatilidad.
```

```{r echo=FALSE, caption='desviacion estandar', warning = FALSE,fig.pos='h',message=FALSE}

tablasd<-merge.data.frame(c(table.AnnualizedReturns(jp_r, scale = 252)[-3,],
table.AnnualizedReturns(jp_r08, scale = 252)[-3,],
table.AnnualizedReturns(jp_r20)[-3,]),"")
row.names(tablasd)<-c("Retorno anualizado","SD anualizado")
colnames(tablasd)<- c("Total","2008-09","2020-21")
tablasd[,4] <- NULL 
tablass<-tablasd
knitr::kable(tablass)
```
En los periodos de crisis se observa una escalada de la volatilidad que viene acompañada de más periodos de volatilidad, lo que puede ser visto como síntoma de clusterización. Particularmente, durante el periodo 2008-2009 los efectos de la crisis parecen ser mayores sobre los retornos que en el periodo 2020-2021 hecho que es consecuente con el efecto apalancamiento, donde a mayor volatilidad y riesgo asociado, se observaron mayores caídas en los precios y consecuentemente menores retornos. 
A partir de la gráfica \ref{1} se requiere modelación del proceso de volatilidad dado que no es constante en el tiempo y se observa clusterización. 

  b. Modelación de la media condicional. ¿Es la media condicional igual a cero?
  Para verificar la necesidad de modelar la media condicional en primer lugar, se examinaron medidas de media incondicional para evaluar la existencia de desviaciones a la media de largo plazo.
```{r echo= FALSE,fig.pos='h',fig.height = 3, fig.width = 7, fig.align = "center", fig.show='hide'}
plot(jp_r20, main= "Media movil y exponencial de retornos USD/JPY", type='l', lwd=1)
lines(SMA(jp_r20, n = 22), col = "darkgreen", lwd=2)
#Observar si hay un proceso de autocorrelación, si la media es cero, la varianza constante, etc.
```

```{r echo=FALSE,fig.pos='h',fig.height = 3, fig.width = 7, fig.align = "center"}
lines(EMA(jp_r20, n = 22), col = "firebrick", lwd=2)
```
Al observar distintas ventanas de tiempo, se determina que es necesario modelar la media de los retornos de la tasa de cambio, debido a la presencia de fluctuaciones con respecto a la media de largo plazo (SMA en verde y EMA en rojo).

Por parsimonia y criterios de información se determian que el modelo a estimar es un $MA(1)$ 
```{r echo=FALSE, fig.pos='h',`results = FALSE`}
ma1<- arma(jp_r,c(0,1))

fit2 <- stats::arima(jp_r, order = c(0,0,1))
knitr:: kable(stargazer(fit2))

```
 
 
 
 
 
  c. Modelación de la varianza condicional (verifique la existencia de efecto leverage o asimetrías comparando diferentes especificaciones usando criterios de información). ¿Cuál es el modelo que mejor se ajusta al proceso de volatilidad de la tasa de cambio?
  

```{r echo=FALSE ,message=FALSE, results = 'asis',fig.pos='h'}
garch11.spec = ugarchspec(variance.model = list(garchOrder=c(1,1)),
                          mean.model = list(armaOrder=c(0,0)))
garch11.fit = ugarchfit(spec=garch11.spec, data=jp_r)

texreg(extract.rugarch(garch11.fit, indep = jp_r, include.rsquared = FALSE), digits = 8, custom.coef.names = c("mu", "omega", "alpha", "beta"), caption = "Resultados del modelo", custom.model.names = "sGARH(1,1)") #for latex # as R^2 is zero in this example.

```

```{r, echo=FALSE,fig.pos='h'}
knitr::kable(signbias(garch11.fit)[,c(2,3)])
```

```{r, echo=FALSE,fig.pos='h'}
nybtab11<-nyblom(garch11.fit)$IndividualStat
colnames(nybtab11)<-c("p-value")
knitr::kable(nybtab11)

nybtab11c<-data.frame(nyblom(garch11.fit)$IndividualCritical)
colnames(nybtab11c)<-c("Valores críticos")
knitr::kable(nybtab11c)
```

Se observa alpha y beta significativos, dan cuenta  de clustering y persistencia.
mu no es significativo, su media es cero.
omega es la constante de la volatilidad, ie, media de largo plazo de la volatilidad del proceso. No significativa.
alpha+beta es casi 1; es un modelo estacionario.
alpha es sesnibilidad a la llegada de información y beta es persistencia de la volatilidad.
Nyblom: hay cambio estructural en mu, alpha y beta. No en omega.
Leverage: no hay conjunto, hay sesgo solamente negativo.
Comparar por bondad de ajuste y criterios de información.

```{r echo=FALSE ,message=FALSE, results = 'asis',fig.pos='h'}
garch11.spec = ugarchspec(variance.model = list(garchOrder=c(1,1)),
                          mean.model = list(armaOrder=c(0,0)))
garch11.fit = ugarchfit(spec=garch11.spec, data=jp_r)

texreg(extract.rugarch(garch11.fit, indep = jp_r, include.rsquared = FALSE), digits = 8, custom.coef.names = c("mu", "omega", "alpha", "beta"), caption = "Resultados del modelo", custom.model.names = "sGARCH(1,1)") #for latex # as R^2 is zero in this example.
```

```{r}
#plot(garch11.fit, which="all")
resid1<-residuals(garch11.fit)
med<-mean(resid1)
normaldens<-data.frame(predicted = resid1, density = dnorm(resid1, mean(resid1), sd(resid1)))
resid1<-residuals(garch11.fit, standardize=T)

grid.arrange(
ggplot(data= as.data.frame(resid1), aes(y =as.data.frame(resid1)[,1], x = index(jp_r))) + geom_line()+ggtitle("Residuos del modelo GARCH(1,1)")+ labs(x = "Fecha",y = "") + theme_minimal(),
ggAcf(resid1,52)+ggtitle("")+labs(x="")+theme_minimal(),
ggplot(data = as.data.frame(resid1), aes(y=..density.., x = jp_r[,1])) +
  geom_histogram(bins = 200) +
  ggtitle("") +
  geom_vline(aes(xintercept=med), linetype="dashed") +
  geom_density(alpha=.2, fill="steelblue") + labs(x='',y='') +
  geom_line(aes(y = density), data = normaldens, colour = "firebrick") + theme_minimal(),
ggPacf(resid1,52)+ggtitle("")+theme_minimal()
)

```

```{r}
		ni = newsimpact(z = NULL, garch11.fit)
		ni.y = ni$zy
		ni.x = ni$zx
		xf = ni$xexpr
		yf  = ni$yexpr
	#	ggplot(data = data.frame(ni.y,ni.x), aes(x=ni.x,y=ni.y))+
	#	  geom_line(colour="steelblue") + xlab(xf) + ylab(yf) + theme_minimal()
#		plot( ni.x, ni.y, ylab = yf, xlab = xf, type = "l", lwd = 2, col = "steelblue", main = "News Impact Curve", cex.main = 0.8)
```

```{r echo=FALSE ,message=FALSE, results = 'asis',fig.pos='h'}
garch11sstd.spec = ugarchspec(variance.model = list(garchOrder=c(1,1)),
                          mean.model = list(armaOrder=c(0,0)),
                          distribution.model = "sstd")
garch11sstd.fit = ugarchfit(spec=garch11sstd.spec, data=jp_r)

texreg(extract.rugarch(garch11sstd.fit, indep = jp_r, include.rsquared = FALSE), digits = 8, custom.coef.names = c("mu", "omega", "alpha", "beta", "skew", "shape"), caption = "Resultados del modelo", custom.model.names = "sGARCH(1,1)~sstd") #for latex # as R^2 is zero in this example.
```

```{r echo=FALSE,message=FALSE, results = 'asis',fig.pos='h'}
#plot(garch11sstd.fit, which="all")
resid2<-residuals(garch11sstd.fit)
med<-mean(resid2)
sstddens1<-data.frame(predicted = resid2, density = dsstd(resid2, mean(resid2), sd(resid2), xi = garch11sstd.fit@fit[["coef"]][["skew"]]))
resid2<-residuals(garch11sstd.fit, standardize=T)

grid.arrange(
ggplot(data= as.data.frame(resid2), aes(y =as.data.frame(resid2)[,1], x = index(jp_r))) + geom_line()+ggtitle("Residuos del modelo GARCH(1,1)~sstd")+ labs(x = "Fecha",y = "") + theme_minimal(),
ggAcf(resid2,52)+ggtitle("")+labs(x="")+theme_minimal(),
ggplot(data = as.data.frame(resid2), aes(y=..density.., x = jp_r[,1])) +
  geom_histogram(bins = 200) +
  ggtitle("") +
  geom_vline(aes(xintercept=med), linetype="dashed") +
  geom_density(alpha=.2, fill="steelblue") + labs(x='',y='') +
  geom_line(aes(y = density), data = sstddens1, colour = "firebrick") + theme_minimal(),
ggPacf(resid2,52)+ggtitle("")+theme_minimal()
)
```

```{r echo=FALSE ,message=FALSE, results = 'asis',fig.pos='h'}
gjrgarch11.spec = ugarchspec(variance.model = list(model="gjrGARCH", garchOrder=c(1,1)),
                          mean.model = list(armaOrder=c(0,0)),
                          distribution.model = "sstd")
gjrgarch11.fit = ugarchfit(spec=gjrgarch11.spec, data=jp_r)

texreg(extract.rugarch(gjrgarch11.fit, indep = jp_r, include.rsquared = FALSE), digits = 8, custom.coef.names = c("mu", "omega", "alpha", "beta", "gamma", "skew", "shape"), caption = "Resultados del modelo", custom.model.names = "GJR-GARCH(1,1)~sstd") #for latex # as R^2 is zero in this example.
```

```{r,message=FALSE, results = 'asis',fig.pos='h'}
resid3<-residuals(gjrgarch11.fit)
med<-mean(resid3)
sstddens2<-data.frame(predicted = resid3, density = dsstd(resid3, mean(resid3), sd(resid3), xi = gjrgarch11.fit@fit[["coef"]][["skew"]]))
resid3<-residuals(gjrgarch11.fit, standardize=T)

params<-as.list(c(mean(resid3), sd(resid3), xi = gjrgarch11.fit@fit[["coef"]][["skew"]]))

grid.arrange(
ggplot(data= as.data.frame(resid3), aes(y =as.data.frame(resid3)[,1], x = index(jp_r))) + geom_line()+ggtitle("Residuos del modelo GJR-GARCH(1,1)~sstd")+ labs(x = "Fecha",y = "") + theme_minimal(),
ggAcf(resid3,52)+ggtitle("")+labs(x="")+theme_minimal(),
ggplot(data = as.data.frame(resid3), aes(y=..density.., x = jp_r[,1])) +
  geom_histogram(bins = 200) +
  ggtitle("") +
  geom_vline(aes(xintercept=med), linetype="dashed") +
  geom_density(alpha=.2, fill="steelblue") + labs(x='',y='') +
  geom_line(aes(y = density), data = sstddens2, colour = "firebrick") + theme_minimal(),
ggplot(data= as.data.frame(resid3), aes(sample = as.data.frame(resid3)[,1])) +
stat_qq(distribution = qsstd, dparams = params) + stat_qq_line(distribution = qsstd, dparams = params, colour="firebrick") + labs(x="",y="") + theme_minimal()
)
```

```{r echo=FALSE, message=FALSE, results = 'asis',fig.pos='h'}
ni_r = newsimpact(z = NULL, gjrgarch11.fit)
		ni.y_r = ni_r$zy
		ni.x_r = ni_r$zx
		xf = ni_r$xexpr
		yf  = ni_r$yexpr
		ggplot(data = data.frame(ni.y_r,ni.x_r, ni.x, ni.y))+
		  geom_line(aes(x=ni.x,y=ni.y, colour="GARCH(1,1)"), color="steelblue") +
		  geom_line(aes(x=ni.x_r,y=ni.y_r, colour="GJR-GARCH(1,1)"), color="firebrick") + 
		  xlab(xf) + ylab(yf) + theme_minimal()
#		plot( ni.x, ni.y, ylab = yf, xlab = xf, type = "l", lwd = 2, col = "steelblue", main = "News Impact Curve", cex.main = 0.8)
```


  d. Pronóstico de la volatilidad a 10 días.

  e. Cuál es la probabilidad de tener pérdidas con el 99% de confianza en un horizonte de día (use los resultados de su modelación para el cálculo).
  
```{r echo= FALSE,message=FALSE} 
# asuminedo distribucion normal
mu_R<- mean(jp_r)
sigma_R<- sd(jp_r)
W0<- 100000
W0*qnorm(0.01, mean= mu_R, sd<- sigma_R)
```
  
  f. ¿Cambió la pandemia del COVID-19 la estructura de la volatilidad de la tasa de cambio? (para eso use datos desde enero del 2018).
```{r}
jp_covid<-window(jp_r, start='2018-01-01', end=Sys.Date())
```
  


